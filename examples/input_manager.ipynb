{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import valenspy as vp\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input Manager\n",
    "The input manager aims to make accessing shared standard datasets easy.\n",
    "As the available data, data path and variables is HPC specific, the input manager only works for HPC systems specified in the dataset_PATHS.yml. To add a dataset or a new machine, add your machine name, datasets and paths to the dataset_PATHS.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = vp.InputManager(machine='hortense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = manager.available_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['dataset'] == 'RADCLIM'].variable.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>variable</th>\n",
       "      <th>base_path</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERA5</td>\n",
       "      <td>pr</td>\n",
       "      <td>/dodrio/scratch/projects/2022_200/project_inpu...</td>\n",
       "      <td>[europe/total_precipitation/hourly/era5-hourly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ERA5-Land</td>\n",
       "      <td>pr</td>\n",
       "      <td>/dodrio/scratch/projects/2022_200/project_inpu...</td>\n",
       "      <td>[belgium/monthly/total_precipitation/era5-land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>EOBS</td>\n",
       "      <td>pr</td>\n",
       "      <td>/dodrio/scratch/projects/2022_200/project_inpu...</td>\n",
       "      <td>[rr_ens_spread_0.1deg_reg_v29.0e.nc, rr_ens_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>CLIMATE_GRID</td>\n",
       "      <td>pr</td>\n",
       "      <td>/dodrio/scratch/projects/2022_200/project_outp...</td>\n",
       "      <td>[PRECIP_QUANTITY_CLIMATE_GRID_1951_2023_daily....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset variable                                          base_path  \\\n",
       "0           ERA5       pr  /dodrio/scratch/projects/2022_200/project_inpu...   \n",
       "32     ERA5-Land       pr  /dodrio/scratch/projects/2022_200/project_inpu...   \n",
       "67          EOBS       pr  /dodrio/scratch/projects/2022_200/project_inpu...   \n",
       "74  CLIMATE_GRID       pr  /dodrio/scratch/projects/2022_200/project_outp...   \n",
       "\n",
       "                                                files  \n",
       "0   [europe/total_precipitation/hourly/era5-hourly...  \n",
       "32  [belgium/monthly/total_precipitation/era5-land...  \n",
       "67  [rr_ens_spread_0.1deg_reg_v29.0e.nc, rr_ens_me...  \n",
       "74  [PRECIP_QUANTITY_CLIMATE_GRID_1951_2023_daily....  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.loc[df['variable'] == 'pr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "For everyday use, a dataset can be accessed through the load data functionality. Variables are accessed through their CORDEX variable name. \n",
    "\n",
    "You do not need to know the how the variable is called in the oroginal dataset. E.g. In era5, the 2m temperature is called 'tp' but is accessed here through the CORDEX variable name 'tas'. The original name is added to the attributes of the variable for reference.\n",
    "\n",
    "Note that the files that are found and used to load the data are printed and the CF_status of the ds is printed. This is to help debug if the data is not loaded as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No files found for dataset ERA5, variables ['pr'], period 2000, frequency daily, region europe and path_identifiers ['min'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERA5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdaily\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meurope\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_identifiers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ds\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/RMIB-UGent/vsc46032_kobe/ValEnsPy/src/valenspy/input_manager.py:153\u001b[0m, in \u001b[0;36mInputManager.load_data\u001b[0;34m(self, dataset_name, variables, period, freq, region, cf_convert, path_identifiers, metadata_info)\u001b[0m\n\u001b[1;32m    144\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_paths(\n\u001b[1;32m    145\u001b[0m     dataset_name,\n\u001b[1;32m    146\u001b[0m     variables\u001b[38;5;241m=\u001b[39mvariables,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     path_identifiers\u001b[38;5;241m=\u001b[39mpath_identifiers,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files found for dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, period \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, frequency \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, region \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and path_identifiers \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_identifiers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile paths found:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No files found for dataset ERA5, variables ['pr'], period 2000, frequency daily, region europe and path_identifiers ['min']."
     ]
    }
   ],
   "source": [
    "ds = manager.load_data(\"ERA5\",[\"pr\"], period=[2000],freq=\"daily\",region=\"europe\", path_identifiers=[\"min\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that depending on your search criteria, metadata is added to the dataset.\n",
    "\n",
    "With the load_dataset functionalilty you can also:\n",
    "- load multiple variables simultaneously and/or\n",
    "- **not** convert the ds to cf-compliant format and/or ``cf_convert=False``\n",
    "- Add additional meta_data to the ds - if using the metdata_info dictionary\n",
    "\n",
    "However, the ds is then not in cf convention and applying diagnostics will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ds = manager.load_data(\"EOBS\",[\"tas\",\"pr\"], path_identifiers=[\"mean\"], cf_convert=True, metadata_info={\"creator\":\"ME\"})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the dataset (e.g. \"ERA5\") should be in the dataset_PATHS.yml file to be able to find the data. If using the inputconvertor ``cf_convert=True`` (default option) a corresponding input_convertor should be available. Currently the following datasets have input convertors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vp.inputconverter.INPUT_CONVERTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A peak inside the manager\n",
    "\n",
    "The input_manager uses the path specified in the dataset_PATHS.yml for the given dataset and machine to search all .nc files and file paths that match the filtering requested. The following function is doing all the \"magic\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "manager._get_file_paths(\"EOBS\",[\"tas\",\"pr\"], path_identifiers=[\"mean\"]) #The magic happens here ! All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above all paths starting with '/dodrio/scratch/projects/2022_200/project_input/External/observations/EOBS/' and containing the original name (long or short name) for 'tas' or 'pr' in this case tg and rr and \"mean\" are selected. Other options are:\n",
    "- region: e.g. europe, belgium\n",
    "- period: [start_year, end_year] possibly more is covered (note some datasets are not stored by year)!\n",
    "- frequency: eg. yearly, daily, monthly\n",
    "- other: Any other keywords to filter by are specified in the path_identifier. E.g. 'mean' for monthly mean data or \"min\" for minimum daily temperatures\n",
    "\n",
    "For more information see the documentation on the input_manager and the load_data function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Load multiple datasets\n",
    "\n",
    "Working on functionality to load multiple datasets at once. Currently this is implemented as load_m_data, but the currently the usage is slightly cluncky and can be improved. The idea is to load multiple datasets at once into a datatree using the load_data functions for each dataset seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_request_dict={\"EOBS\":\n",
    "                        {\"path_identifiers\":[\"mean\"]},\n",
    "                    \"ERA5\":\n",
    "                        {\"period\":[2000,2001],\n",
    "                         \"freq\":\"daily\",\n",
    "                         \"region\":\"europe\",\n",
    "                         \"path_identifiers\":[\"min\"]}}\n",
    "\n",
    "\n",
    "dt = manager.load_m_data(data_request_dict, variables=[\"tas\",\"pr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tests of input manager \n",
    "Finding exceptional and rare cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EOBS finding mean and spread files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "manager._get_file_paths(\"EOBS\",[\"tas\"], path_identifiers=[],) #The magic happens here ! All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ERA5 not giving only \"mean\" values when also min and max exist (due to naming of files!)\n",
    "-> I don't think we need to resolve this here? But rather when we give the ERA5 data a new structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "manager._get_file_paths(\"ERA5\",[\"tas\"], period=[2000,2001],freq=\"daily\",region=\"europe\") #The magic happens here ! All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same is true for ERA5-Land, here different values for pr (mean, min and max) are loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ds = manager.load_data(\"ERA5-Land\",[ \"pr\",\"hfls\"], period=[2000,2001], freq=\"daily\", region=\"belgium\", path_identifiers=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'valenspy_dev (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
