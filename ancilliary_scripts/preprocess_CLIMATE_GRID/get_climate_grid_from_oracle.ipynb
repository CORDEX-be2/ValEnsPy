{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get CLIMATE_GRID data from the RMI Oracle database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# PREPROCESS CLIMATE_GRID\n",
    "\n",
    "# Script to download and grid CLIMATE_GRID data into netcdf files\n",
    "# \n",
    "#   Step 1: connect to RMI Oracle DB and download raw data into .csv files (only works on kili)\n",
    "#\n",
    "#   Step 2: Grid the raw .csv files into netcdf files for further use. \n",
    "#\n",
    "# I. Vanderkelen, June 2024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "import oracledb\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from datetime import date\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# User options: download raw oracle data. If false, load intermediate .csv files\n",
    "download_from_oracle = True\n",
    "\n",
    "# user settings\n",
    "data_dir = '/scratch/invander/CLIMATE_GRID/'\n",
    "dataset = \"CLIMATE_GRID\"\n",
    "\n",
    "\n",
    "variables = [\"EVAPOTRANS_REF\"] #, \"SUN_INT\", \"SUN_DURATION\", \"PRECIP_DURATION\", \"WIND_PEAK_SPEED\", \"PRECIP_1H_MAX\", \"EVAPOTRANS_REF\", \"TEMP_MAX\",\"HUMIDITY_RELATIVE\",\"TEMP_MIN\", \"TEMP_AVG\", \"WIND_SPEED\", \"PRESSURE\", \"SHORT_WAVE_FROM_SKY\", \"SUN_INT_HORIZ\", \"PRECIP_QUANTITY\"]\n",
    "\n",
    "# init.yr & end.yr: both init.yr and end.yr are included in the request.\n",
    "init_yr = 1950\n",
    "end_yr = 2023\n",
    "\n",
    "\n",
    "# do it for all variables\n",
    "for variable in variables:\n",
    "    \n",
    "\n",
    "    # ----------------------------------------\n",
    "    # PART 1: Get data from RMI Oracle database\n",
    "\n",
    "\n",
    "    # define filenames of intermediate files\n",
    "    filename_csv = 'climate_atlas_'+variable+'_'+dataset+'_'+str(init_yr)+'_'+str(end_yr)+'.csv'\n",
    "    \n",
    "    filename_municipalities_csv = 'climate_atlas_'+variable+'_'+dataset+'_municipalities_'+str(init_yr)+'_'+str(end_yr)+'.csv'\n",
    "\n",
    "    if download_from_oracle: \n",
    "        \n",
    "        # oracle database connection\n",
    "        username = getpass.getpass(\"Enter oracle username: \")\n",
    "        password = getpass.getpass(\"Enter password: \")\n",
    "        \n",
    "        host = \"delphi.oma.be\"\n",
    "        service_name = \"rmidbs1.oma.be\"\n",
    "        port=1521\n",
    "\n",
    "        params = oracledb.ConnectParams(host=host, port=port, service_name=service_name)\n",
    "        connection = oracledb.connect(user=username, password=password, params=params)\n",
    "\n",
    "        init_date = f\"{init_yr}0101\"\n",
    "        end_date = f\"{end_yr + 1}0101\"\n",
    "\n",
    "        # get info from grid\n",
    "        df_grid = pd.read_sql(\"SELECT PIXEL_ID, PIXEL_LON_CENTER, PIXEL_LAT_CENTER FROM CLIMATE_GRID_PIXEL ORDER BY PIXEL_ID\", connection)\n",
    "        pixel_ids = df_grid['PIXEL_ID'].tolist()\n",
    "\n",
    "        # get info from municipalities\n",
    "        df_municipality_ids = pd.read_sql(\"SELECT NAME, CODE_INS from MUNICIPALITY\", connection)\n",
    "        municipalities_codes = df_municipality_ids['CODE_INS'].tolist()\n",
    "\n",
    "\n",
    "        print(f\"Oracle climate grid query for variable: {variable}\")\n",
    "\n",
    "        req_str = f\"\"\"\n",
    "            SELECT CODE_PIXEL_ID_CODE_INS, DATE_END, {variable}\n",
    "            FROM {dataset}\n",
    "            WHERE DATE_END = DATE_BEGIN\n",
    "            AND DATE_BEGIN >= TO_DATE('{init_date}', 'YYYYMMDD')\n",
    "            AND DATE_END < TO_DATE('{end_date}', 'YYYYMMDD')\n",
    "            ORDER BY DATE_END, CODE_PIXEL_ID_CODE_INS\n",
    "        \"\"\"\n",
    "        \n",
    "        df_data = pd.read_sql(req_str, connection)\n",
    "\n",
    "        # close the connection\n",
    "        connection.close()\n",
    "\n",
    "        df_data.columns = [\"location\", \"time\", \"value\"]\n",
    "        df_data['time'] = pd.to_datetime(df_data['time']).dt.date\n",
    "\n",
    "        # speparate grid cells and municipalities\n",
    "        df_gridcells = df_data[df_data['location'].isin(pixel_ids)]\n",
    "        df_municipality = df_data[df_data['location'].isin(municipalities_codes)]\n",
    "\n",
    "        # save intermediate data\n",
    "        df_gridcells.to_csv(data_dir+filename_csv)\n",
    "\n",
    "        df_municipality.to_csv(data_dir+filename_municipalities_csv)\n",
    "\n",
    "    else: \n",
    "        print(\"Loading csv file: \"+data_dir+filename_csv)\n",
    "        # load intermediate csv - for now only grid cells are used. \n",
    "        df_gridcells     = pd.read_csv(data_dir+filename_csv)\n",
    "\n",
    "\n",
    "    # prepare for gridding into netcdf\n",
    "    df_pivotted = df_gridcells.pivot_table(index='location', columns=['time'], values='value', fill_value=np.nan)\n",
    "    data = df_pivotted.values\n",
    "    dates = pd.to_datetime(df_pivotted.columns)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # PART 2: Regridding into netcdf files\n",
    "\n",
    "\n",
    "    # INFO ON GRID\n",
    "    # load climategrid meta data on variables and units\n",
    "    meta = pd.read_csv('CLIMATE_GRID_meta.csv', delimiter=\";\")\n",
    "\n",
    "    # load information on projection used \n",
    "    proj_string = \"+proj=lcc +lat_1=49.83333388888889 +lat_2=51.16666722222222 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs\"\n",
    "    # load the pixel lat and lon variable and use this to transpose to own defined grid\n",
    "    df_coords_points = pd.read_csv('grid_5kmx5km.csv', header=1, delimiter=' ') # these are the lat lons and lambert coordinates for all pixels in CLIMATE_DATA\n",
    "\n",
    "\n",
    "    # load the full grid, creased based on the proj_string and following bounding points:\n",
    "    #NE_lon, NE_lat = 9.53269211610237, 53.4367017362904\n",
    "    #SW_lon, SW_lat = 0.163155782953472, 47.515819098539\n",
    "    df_full_grid = pd.read_csv('lambert_coordinates_full_climate_grid.csv') # made using R script Michel Journee\n",
    "    lambert_x_grid_raw = df_full_grid['x1'].unique()\n",
    "    lambert_y_grid_raw = df_full_grid['x2'].unique()\n",
    "\n",
    "    # cut this grid to boundingbox including gridcells from CLIMATE_GRID\n",
    "    lambert_x_grid_cutlow = lambert_x_grid_raw[lambert_x_grid_raw>=df_coords_points['LAMBERT_X'].min()]\n",
    "    lambert_x_grid = lambert_x_grid_cutlow[lambert_x_grid_cutlow<=  df_coords_points['LAMBERT_X'].max()]\n",
    "    lambert_y_grid_cutlow = lambert_y_grid_raw[lambert_y_grid_raw>=df_coords_points['LAMBERT_Y'].min()]\n",
    "    lambert_y_grid = lambert_y_grid_cutlow[lambert_y_grid_cutlow<=  df_coords_points['LAMBERT_Y'].max()]\n",
    "\n",
    "\n",
    "    # Find the nearest index in the lons and lats grids and add this to coordinates dataframe\n",
    "    def find_nearest(array, value):\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    df_coords_points['LAMBERT_X_INDEX'] = df_coords_points['LAMBERT_X'].apply(lambda x: find_nearest(lambert_x_grid, x))\n",
    "    df_coords_points['LAMBERT_Y_INDEX'] = df_coords_points['LAMBERT_Y'].apply(lambda x: find_nearest(lambert_y_grid, x))\n",
    "\n",
    "\n",
    "\n",
    "    print('Converting to netcdf: '+ variable)\n",
    "\n",
    "    # load the time data and convert to dates\n",
    "\n",
    "    # load the vector data\n",
    "    #data = robjects.r['grid.vec']\n",
    "\n",
    "    # create empty array to fill with gridded data - also for lat and lon\n",
    "    grid_data = np.full(( len(dates),  len(lambert_y_grid),len(lambert_x_grid)  ), np.nan)\n",
    "\n",
    "    lat_2d = np.full((len(lambert_y_grid),len(lambert_x_grid)  ), np.nan)\n",
    "    lon_2d = np.full((len(lambert_y_grid),len(lambert_x_grid)  ), np.nan)\n",
    "\n",
    "    # Fill the grid data array\n",
    "    for index, row in df_coords_points.iterrows():\n",
    "        lambert_x_idx = int(row['LAMBERT_Y_INDEX'])\n",
    "        lambert_y_idx = int(row['LAMBERT_X_INDEX'])\n",
    "\n",
    "        pixel_id = int(row['PIXEL_ID'])\n",
    "        \n",
    "        grid_data[:, lambert_x_idx, lambert_y_idx ] = data[int(pixel_id) - 1, :]\n",
    "\n",
    "        lat_2d[ lambert_x_idx, lambert_y_idx ] = df_coords_points[df_coords_points['PIXEL_ID'] == pixel_id][\"LAT\"].values[0]\n",
    "        lon_2d[ lambert_x_idx, lambert_y_idx ] = df_coords_points[df_coords_points['PIXEL_ID'] == pixel_id][\"LON\"].values[0]\n",
    "\n",
    "\n",
    "    # get metadata from meta dataframe\n",
    "    unit = meta.loc[meta['variable'] == variable, 'unit'].values[0] \n",
    "    long_name = meta.loc[meta['variable'] == variable, 'long_name'].values[0] \n",
    "    description = meta.loc[meta['variable'] == variable, 'description'].values[0] \n",
    "\n",
    "    # create data array\n",
    "    da = xr.DataArray(\n",
    "        data=grid_data,\n",
    "        dims=[\"time\", \"y\", \"x\"],\n",
    "        coords=dict(\n",
    "            y=lambert_y_grid,\n",
    "            x=lambert_x_grid,\n",
    "            time=dates,\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            long_name=long_name,\n",
    "            description = description,\n",
    "            units=unit,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    da['x'].attrs = {'units':\"E[east]: Easting (meters)\", 'long_name': \" x coordinate Lambert Conic Conformal (2SP)\"}\n",
    "    da['y'].attrs = {'units':\"N[north]: Northing (meters)\", 'long_name': \"y coordinate Lambert Conic Conformal (2SP)\"}\n",
    "\n",
    "    # convert to dataset and give dataset attributes\n",
    "    ds = da.to_dataset(name=variable)\n",
    "\n",
    "    # add 2d lat and lon in lambert coordinates as well\n",
    "    ds[\"lat\"] = xr.DataArray(\n",
    "        data=lat_2d,\n",
    "        dims=[\"y\", \"x\"],\n",
    "        coords=dict(\n",
    "            y=lambert_y_grid,\n",
    "            x=lambert_x_grid,\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            long_name=\"latitude\",\n",
    "            description = \"WGS84 latitude, from values of CLIMATE_GRID, provided per grid point\",\n",
    "            units=\"degrees_north\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # interpolatie to also have lat values outside of Belgium\n",
    "    ds['lat'] = ds['lat'].interpolate_na(dim='x', method='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "    ds[\"lon\"] = xr.DataArray(\n",
    "        data=lon_2d,\n",
    "        dims=[\"y\", \"x\"],\n",
    "        coords=dict(\n",
    "            y=lambert_y_grid,\n",
    "            x=lambert_x_grid,\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            long_name=\"longitude\",\n",
    "            description = \"WGS84 longitude, from values of CLIMATE_GRID, provided per grid point\",\n",
    "            units=\"degrees_east\",\n",
    "        ),\n",
    "    )\n",
    "    d_attrs = {\"creation_date\": date.today().strftime(\"%d-%m-%Y\"),\n",
    "    \"creators\": \"Ghilain N., Van Schaeybroeck B., Vanderkelen I.\", \n",
    "    \"contact\": \"inne.vanderkelen@meteo.be\",\n",
    "    \"version\": \"1.1\", \"affiliation\": \"Royal Meteorological Institute of Belgium\", \n",
    "    \"projection\":proj_string}\n",
    "    ds.attrs = d_attrs\n",
    "\n",
    "    # interpolatie to also have lon values outside of Belgium\n",
    "    ds['lon'] = ds['lon'].interpolate_na(dim='x', method='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "    # also pass crs using rioxarray - passing the spatial_ref\n",
    "    ds.rio.write_crs(ccrs.Projection(proj_string), inplace=True)\n",
    "\n",
    "    # export to netcdf\n",
    "    filename_out = str(variable)+'_CLIMATE_GRID_'+str(dates.year.min())+'_'+str(dates.year.max())+'_daily.nc'\n",
    "    ds.to_netcdf(data_dir + filename_out, encoding={'time':  {'dtype': 'int32'} })\n",
    "    print('Saved as: '+data_dir + filename_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle climate grid query for variable: EVAPOTRANS_REF\n",
      "Converting to netcdf: EVAPOTRANS_REF\n",
      "Saved as: /scratch/invander/CLIMATE_GRID/EVAPOTRANS_REF_CLIMATE_GRID_1961_2023_daily.nc\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valenspy_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
